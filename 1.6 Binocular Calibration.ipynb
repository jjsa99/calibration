{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e8bc8a8-b00e-4a32-bdff-15321e1dcf4d",
   "metadata": {},
   "source": [
    "# 1. Stereo Calibration\n",
    "This script intends to do stereo calibration using Zhang's method manually.\n",
    "The images used were part of a Github(add github)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5b4cd-f5cc-449e-9978-8e2a9d594aea",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bb7827-6f93-4462-befc-686fa18d8370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "#for LM method\n",
    "from scipy.optimize import curve_fit\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf62e6-b261-4fed-b2e7-5f28d651e8eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Zhang's Method\n",
    "Comparing to the DLT method, the Zhang method only calculate the intrinsic parameters. We instead only need to find 5 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28118a15-aaa8-4c08-ba22-1c0fa570fb9a",
   "metadata": {},
   "source": [
    "## Normalize matrix\n",
    "Normalization of input data. To improve numerical stability of the calculations, it is recommended to normalize both 2D point sets X and x before performing the homography estimation. Burger[pag.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222b7adc-7fd6-4d67-8526-ddc0d7af41da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing a given 2D point set is accomplished by shifting and scaling all points such that the centroid od the \n",
    "# transformed set is aligned with the origin and its diameter has a predefined size.\n",
    "# There are various methods in the literature to calculate the normalization. In this case, the method used is \n",
    "# applied scaling non-unformly in x- and y- direction such that the variances along both axis get normalized. Burger[appendix B, pag 46]\n",
    "def matrix_normalization(data):\n",
    "    \"\"\"\n",
    "       Args:\n",
    "          data: Nx2 stack of data points\n",
    "        Returns:\n",
    "          The normalization matrix\n",
    "    \"\"\"\n",
    "    if data.ndim != 2 or data.shape[-1] != 2:\n",
    "        raise ValueError('Dataset must be a collection of 2D points')\n",
    "\n",
    "    x, y = data[:, 0], data[:, 1]\n",
    "\n",
    "    N = data.shape[0]\n",
    "\n",
    "    x_mean, y_mean = x.mean(), y.mean()\n",
    "    x_var, y_var = x.var(), y.var()\n",
    "    \n",
    "    # Form rescaling matrix so that data points will lie\n",
    "    # sqrt(2) from the origin on average.\n",
    "    s_x, s_y = np.sqrt(2. / x_var), np.sqrt(2. / y_var)\n",
    "    \n",
    "    norm_matrix = np.array([[s_x,  0., -s_x * x_mean],\n",
    "                            [ 0., s_y, -s_y * y_mean],\n",
    "                            [ 0.,  0.,            1.]])\n",
    "\n",
    "    return norm_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1613f9-1f62-4a9e-aa30-59b8a9bf457d",
   "metadata": {},
   "source": [
    "## Obtain the Corner information of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77bbeb-5fc9-403c-ab73-fdbca75aa60b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Finding the corner of the checkerboard\n",
    "+ With 9 images, each one with 48 image points\n",
    "+ The checkerboard has 25mm between corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee32301a-204e-4f88-9a2d-771d0ff6d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cornerfinding(n_imgs,patternsize,dist, img_name):\n",
    "    '''\n",
    "    @param\n",
    "    [in] n_imgs : number of images used\n",
    "    [in] patternsize : size of the checkerboard\n",
    "    [in] dist : distance between squares\n",
    "    [in] img_name : name of the jpg\n",
    "    [out] imagecoo : imagecoordinates\n",
    "    [out] worldcoo : worldcoordinates\n",
    "    '''\n",
    "    # number of images\n",
    "    n_imgs = n_imgs\n",
    "    # pattern size\n",
    "    patternsize= patternsize\n",
    "    #distance between squares(in mm)\n",
    "    dist = dist\n",
    "\n",
    "    # specified number of iterations are completed.\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS +\n",
    "                cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "    # Vector for 3D points\n",
    "    threedpoints = []\n",
    "\n",
    "    # Vector for 2D points\n",
    "    twodpoints = []\n",
    "\n",
    "    #3D points real world coordinates( static -> chessboard)\n",
    "    # create a matrix of zeros for 48 points , each with 3 coordinates\n",
    "    world_coo = np.zeros((1, patternsize[0]* patternsize[1],3), np.float32)\n",
    "    world_coo[:, :, :2] = np.mgrid[0:patternsize[0], 0:patternsize[1]].T.reshape(-1, 2)\n",
    "    # Transform coordinates in mm\n",
    "    world_coo = world_coo*dist\n",
    "\n",
    "    #prev_img_shape = None\n",
    "\n",
    "    for i in range(n_imgs):\n",
    "\n",
    "        #Import the image\n",
    "        name = (str(img_name)+ str(0) + str(i+1)+\".jpg\")\n",
    "        img = cv2.imread(name)\n",
    "        # convert it to gray scale\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        retval, corners= cv2.findChessboardCorners(gray,patternsize,cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK +cv2.CALIB_CB_NORMALIZE_IMAGE)\n",
    "\n",
    "        if(retval == True):\n",
    "            threedpoints.append(world_coo)\n",
    "\n",
    "            # Refining pixel coordinates or given 2d points.\n",
    "            corners2 = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria).reshape(patternsize[0]*patternsize[1],2)\n",
    "\n",
    "            twodpoints.append(corners2)\n",
    "            # Draw and display the corners\n",
    "            image = cv2.drawChessboardCorners(img,patternsize,corners2,retval)\n",
    "            #plt.imshow(image)\n",
    "            #plt.show()\n",
    "\n",
    "    # image coordinates and world coordinates for the diferent images( the world coordinates are always the same)\n",
    "    imagecoo = np.asarray(twodpoints)\n",
    "    worldcoo = np.asarray(threedpoints) \n",
    "    return imagecoo, worldcoo\n",
    "\n",
    "name_left = (\"Research/Stereo-Calibration-master/Stereo-Calibration-master/pic/left/left\")\n",
    "name_right = (\"Research/Stereo-Calibration-master/Stereo-Calibration-master/pic/right/right\")\n",
    "n_imgs = 9\n",
    "square_size = 25\n",
    "pattern_size = (9,6)\n",
    "imagecoo_left,worldcoo_left = cornerfinding(n_imgs,pattern_size,square_size,name_left)\n",
    "imagecoo_right,worldcoo_right = cornerfinding(n_imgs,pattern_size,square_size,name_right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f511e2ca-ffe3-4df9-ace2-a67da4f055cc",
   "metadata": {},
   "source": [
    "# 3. Homography estimation with the DLT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee1c11-4f43-4ec1-b0d6-f6509879036c",
   "metadata": {},
   "source": [
    "## 3.1 Solving homogeneous systems of linear equations: **SVD**\n",
    "**goal : find the homography matrix (H)** \n",
    "- Normalization matrix\n",
    "- Apply the matrix to the coordinates\n",
    "- Homogenous coordinates transformation\n",
    "- Having a system M*h = , we apply SVD(M). \n",
    "- Denormalize the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1967e91a-9a8d-42d2-b87f-2b11b8e49b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform coordinates to euclidean\n",
    "def to_euclidean(homogeneous_coordinate):\n",
    "    euclidean = homogeneous_coordinate[:,:]/(homogeneous_coordinate[:,-1][:,np.newaxis])\n",
    "\n",
    "    return euclidean[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "500c9fb5-92fe-4c6c-b1ec-882bb84c6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform coordinates to homogenous\n",
    "def to_homogeneous(euclideancoo):\n",
    "\n",
    "    if euclideancoo.ndim != 2 or euclideancoo.shape[-1] != 2:\n",
    "        raise ValueError('Stacked vectors must be 2D inhomogeneous')\n",
    "    \n",
    "    M,N = euclideancoo.shape\n",
    "    homogeneous = np.hstack(((euclideancoo),np.ones((M,1))))\n",
    "    \n",
    "    return homogeneous\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbea4b3f-49d5-4e81-867e-91822f215e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homography_matrix(worldcoo,imagecoo):\n",
    "    # get the shape of the matrix\n",
    "    worldm, worldn = worldcoo.shape\n",
    "    imagem, imagen = imagecoo.shape\n",
    "\n",
    "    homo_world = to_homogeneous(worldcoo) \n",
    "    homo_image = to_homogeneous(imagecoo)\n",
    "\n",
    "    # world coordinate normalization\n",
    "    X_matrix_norm = matrix_normalization(worldcoo)\n",
    "    # image coordinate normalization\n",
    "    x_matrix_norm = matrix_normalization(imagecoo)\n",
    "    \n",
    "    # Normalize the coordinates\n",
    "    world_norm = np.dot(homo_world,X_matrix_norm.T)\n",
    "    image_norm = np.dot(homo_image,x_matrix_norm.T)\n",
    "\n",
    "    #2D DLT\n",
    "    X, Y, x, y = world_norm[:,0],world_norm[:,1], image_norm[:,0], image_norm[:,1]\n",
    "\n",
    "    A = np.zeros((worldm * 2, 9))\n",
    "\n",
    "    M_x = np.zeros((worldm, 9))\n",
    "    M_x[:, 0] = -X\n",
    "    M_x[:, 1] = -Y\n",
    "    M_x[:, 2] = -1.\n",
    "    M_x[:, 6] =  x * X\n",
    "    M_x[:, 7] =  x * Y\n",
    "    M_x[:, 8] =  x\n",
    "\n",
    "    M_y = np.zeros((worldm, 9))\n",
    "    M_y[:, 3] = -X\n",
    "    M_y[:, 4] = -Y\n",
    "    M_y[:, 5] = -1.\n",
    "    M_y[:, 6] =  y * X\n",
    "    M_y[:, 7] =  y * Y\n",
    "    M_y[:, 8] =  y\n",
    "\n",
    "    # Note that all x-constraints precede all y-constraints for convenience of \n",
    "    # representation.\n",
    "    A[:worldm] = M_x\n",
    "    A[worldm:] = M_y\n",
    "\n",
    "    U, S, V = np.linalg.svd(A)\n",
    "    # get the min Singular value\n",
    "    idx = np.argmin(S)\n",
    "    H = V[idx].reshape((3,3))\n",
    "    \n",
    "    # Denormalize the coordinates\n",
    "    H = np.dot(np.dot(np.linalg.inv(x_matrix_norm), H), X_matrix_norm)\n",
    "\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572d0aa-99fa-4cd7-a586-b9421409b29e",
   "metadata": {},
   "source": [
    "## 3.1.1 Homography matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aaf4646d-3a54-4c29-a10e-422bc055d970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.07311460e+00,  8.05182666e-02,  2.43963820e+02],\n",
       "        [-8.31549176e-02,  1.34583618e+00,  9.20397492e+01],\n",
       "        [-5.51128424e-04,  1.99210057e-04,  1.00000000e+00]],\n",
       "\n",
       "       [[-4.70267037e-01,  1.41998099e+00,  2.54364227e+02],\n",
       "        [-1.56160948e+00,  2.69462684e-01,  3.60168250e+02],\n",
       "        [-1.80593240e-03, -1.32304976e-04,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.35401909e+00, -9.06425593e-01,  2.77263943e+02],\n",
       "        [ 4.05548002e-01,  1.29316608e+00,  6.87908911e+01],\n",
       "        [-4.95035975e-04, -8.73772228e-04,  1.00000000e+00]],\n",
       "\n",
       "       [[ 1.30128758e+00, -1.45226272e-01,  1.86812380e+02],\n",
       "        [-1.83201826e-01,  1.48789605e+00,  1.28603805e+02],\n",
       "        [-6.78734312e-04, -3.78809805e-04,  1.00000000e+00]],\n",
       "\n",
       "       [[-1.64152277e-01, -1.60961398e+00,  4.38376577e+02],\n",
       "        [ 1.07892125e+00,  3.73532858e-01,  4.74300114e+01],\n",
       "        [-1.40786136e-03, -5.83203940e-05,  1.00000000e+00]],\n",
       "\n",
       "       [[-3.51539746e-02, -1.12166762e+00,  5.92433346e+02],\n",
       "        [ 1.53832073e+00, -2.51506246e-02,  1.39013200e+02],\n",
       "        [ 2.88984006e-04,  6.53474397e-04,  1.00000000e+00]],\n",
       "\n",
       "       [[-4.43105698e-01, -8.86364019e-01,  3.69501507e+02],\n",
       "        [ 1.31032940e+00, -1.46139697e-01,  1.36598063e+02],\n",
       "        [ 8.83085794e-06,  1.06922379e-03,  1.00000000e+00]],\n",
       "\n",
       "       [[-7.36667912e-01, -1.28831295e+00,  4.73255358e+02],\n",
       "        [ 1.28647437e+00, -6.98635546e-02,  9.04075071e+01],\n",
       "        [-9.78129127e-04,  8.45824461e-04,  1.00000000e+00]],\n",
       "\n",
       "       [[ 2.20040572e+00, -1.53612132e-01,  2.18189465e+02],\n",
       "        [ 5.11881625e-01,  1.93596804e+00,  8.32581864e+01],\n",
       "        [ 1.49083514e-03,  4.74837621e-04,  1.00000000e+00]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def homo_calculation(n_imgs,patternsize,imagecoo,worldcoo):\n",
    "\n",
    "    # empty array to store the homographies\n",
    "    H = np.zeros((n_imgs,3,3))\n",
    "    # empty array to store the normalized homographies\n",
    "    H_norm= np.zeros((n_imgs,3,3))\n",
    "    # world coordinates\n",
    "    wo = worldcoo[0][:].reshape(patternsize[0]*patternsize[1],3)\n",
    "    # remove the Z coordinate\n",
    "    wo = wo[:,:2]\n",
    "\n",
    "    for i in range(n_imgs):\n",
    "        im = imagecoo[i][:][:].reshape(patternsize[0]*patternsize[1],2)\n",
    "        # homography matrix\n",
    "        H[i,:] = homography_matrix(wo,im)\n",
    "        # normalized homography matrix\n",
    "        H_norm[i,:] = H[i,:]/H[i,-1,-1]\n",
    "\n",
    "    return H_norm \n",
    "\n",
    "\n",
    "H_norm_left = homo_calculation(n_imgs,pattern_size,imagecoo_left,worldcoo_left)\n",
    "H_norm_right = homo_calculation(n_imgs,pattern_size,imagecoo_right,worldcoo_right)\n",
    "\n",
    "H_norm_left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c9e02b-ad92-4696-8f94-61474fd369ef",
   "metadata": {},
   "source": [
    "## 3.1.2 Homagraphy matrix refinement\n",
    "Non-linear refiment of the homography matrix using **Levenberg-Marquardt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c43e4-b903-4540-9d6a-26a37fc23e2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aux functions\n",
    "**a) jacobian refine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ae0cfe06-3f8b-475b-b93b-4200f836aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac_refine(xdata, *params):\n",
    "    \"\"\"Jacobian function for Levenberg-Marquardt refinement.\n",
    "    \"\"\"\n",
    "    h11, h12, h13, h21, h22, h23, h31, h32, h33 = params\n",
    "\n",
    "    N = xdata.shape[0] // 2\n",
    "\n",
    "    X = xdata[:N]\n",
    "    Y = xdata[N:]\n",
    "\n",
    "    J = np.zeros((N * 2, 9))\n",
    "    J_x = J[:N]\n",
    "    J_y = J[N:]\n",
    "\n",
    "    s_x = h11 * X + h12 * Y + h13\n",
    "    s_y = h21 * X + h22 * Y + h23\n",
    "    w   = h31 * X + h32 * Y + h33\n",
    "    w_sq = w**2\n",
    "\n",
    "    J_x[:, 0] = X / w\n",
    "    J_x[:, 1] = Y / w\n",
    "    J_x[:, 2] = 1. / w\n",
    "    J_x[:, 6] = (-s_x * X) / w_sq\n",
    "    J_x[:, 7] = (-s_x * Y) / w_sq\n",
    "    J_x[:, 8] = -s_x / w_sq\n",
    "\n",
    "    J_y[:, 3] = X / w\n",
    "    J_y[:, 4] = Y / w\n",
    "    J_y[:, 5] = 1. / w\n",
    "    J_y[:, 6] = (-s_y * X) / w_sq\n",
    "    J_y[:, 7] = (-s_y * Y) / w_sq\n",
    "    J_y[:, 8] = -s_y / w_sq\n",
    "\n",
    "    J[:N] = J_x\n",
    "    J[N:] = J_y\n",
    "\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d5a30-f654-40c4-bf21-36102ebd49a3",
   "metadata": {},
   "source": [
    "**b) Value function LM refinement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a12a84d-0695-4ab2-893f-addf93d7b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_refine(xdata, *params):\n",
    "    \"\"\"Value function for Levenberg-Marquardt refinement.\n",
    "    \"\"\"\n",
    "    h11, h12, h13, h21, h22, h23, h31, h32, h33 = params\n",
    "\n",
    "    N = xdata.shape[0] // 2\n",
    "\n",
    "    X = xdata[:N]\n",
    "    Y = xdata[N:]\n",
    "\n",
    "    x = (h11 * X + h12 * Y + h13) / (h31 * X + h32 * Y + h33)\n",
    "    y = (h21 * X + h22 * Y + h23) / (h31 * X + h32 * Y + h33)\n",
    "\n",
    "    result = np.zeros_like(xdata)\n",
    "    result[:N] = x\n",
    "    result[N:] = y\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176599ed-3c6f-4067-81f7-0c85303eaf3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "**c) Nonlinear LS to refine linear homography estimates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68acf3fc-e9f8-4e60-8994-ea23543c089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_homography(H,worldcoo,imagecoo):\n",
    "    \"\"\"\n",
    "    Performs nonlinear LS to refine linear homography estimates.\n",
    "    \n",
    "    Args:\n",
    "        H : 3x3 homography matrix\n",
    "        worldcoo : Nx2 world coordinates\n",
    "        imagecoo : Nx2 image coordinates\n",
    "    Returns:\n",
    "        Refined 3x3 homography\n",
    "    \"\"\"\n",
    "    X,Y,x,y = worldcoo[:,0],worldcoo[:,1],imagecoo[:,0],imagecoo[:,1]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    h0 = H.ravel()\n",
    "\n",
    "    xdata = np.zeros(N * 2)\n",
    "    xdata[:N] = X\n",
    "    xdata[N:] = Y\n",
    "\n",
    "    ydata = np.zeros(N * 2)\n",
    "    ydata[:N] = x\n",
    "    ydata[N:] = y\n",
    "\n",
    "    # Use Levenberg-Marquardt to refine the linear homography estimate\n",
    "    popt, pcov = curve_fit(f_refine, xdata, ydata, p0=h0, jac=jac_refine)\n",
    "    h_refined = popt\n",
    "    \n",
    "    # Normalize and reconstitute homography\n",
    "    h_refined /= h_refined[-1]\n",
    "    H_refined = h_refined.reshape((3,3))\n",
    "\n",
    "    return H_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb94378-4443-4ec0-b617-2ffe0d43d7a4",
   "metadata": {},
   "source": [
    "## 3.1.3 Homography matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fde406ff-dc08-4291-8a9d-8c397e8f0be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def homo_calculation(n_imgs,patternsize,imagecoo,worldcoo):\n",
    "    # empty array to store the homographies\n",
    "    H_refined = np.zeros((n_imgs,3,3))\n",
    "    # temporary array \n",
    "    H_temp = np.array((3,3))\n",
    "    # world coordinates\n",
    "    wo = worldcoo[0][:].reshape(patternsize[0]*patternsize[1],3)\n",
    "    # remove the Z coordinate\n",
    "    wo = wo[:,:2]\n",
    "\n",
    "    for i in range(n_imgs):\n",
    "        im = imagecoo[i][:][:].reshape(patternsize[0]*patternsize[1],2)\n",
    "        H_temp = homography_matrix(wo,im)\n",
    "        H_refined[i,:] = refine_homography(H_temp,wo,im)\n",
    "    \n",
    "    return H_refined\n",
    "\n",
    "H_refined_left = homo_calculation(n_imgs,pattern_size,imagecoo_left,worldcoo_left)\n",
    "H_refined_right = homo_calculation(n_imgs,pattern_size,imagecoo_right,worldcoo_right)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a7dfeb-c33a-4df1-997f-87ca7b6fe7d3",
   "metadata": {},
   "source": [
    "## 4. Determining the intrinsic camera parameters\n",
    "Use the computed homographies to calculate the intrisic matrix \\\n",
    "Since H has **8DoF, it's needed >= 4 points to estimate the homography**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28d679-4d75-45aa-92cf-fd362b9bf600",
   "metadata": {},
   "source": [
    "### 4.1 Generate $V_{pq}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33a6de93-60cf-4a24-b6cc-b3f5da2c25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_v(H_stack, i, j):\n",
    "    \"\"\"Generate intrinsic orthogonality constraints. See Zhang pg. 6 for\n",
    "       details.\n",
    "    \"\"\" \n",
    "    M = H_stack.shape[0]\n",
    "\n",
    "    v_ij = np.zeros((M, 6))\n",
    "    v_ij[:, 0] = H_stack[:, 0, i] * H_stack[:, 0, j]\n",
    "    v_ij[:, 1] = H_stack[:, 0, i] * H_stack[:, 1, j] + H_stack[:, 1, i] * H_stack[:, 0, j]\n",
    "    v_ij[:, 2] = H_stack[:, 1, i] * H_stack[:, 1, j]\n",
    "    v_ij[:, 3] = H_stack[:, 2, i] * H_stack[:, 0, j] + H_stack[:, 0, i] * H_stack[:, 2, j]\n",
    "    v_ij[:, 4] = H_stack[:, 2, i] * H_stack[:, 1, j] + H_stack[:, 1, i] * H_stack[:, 2, j]\n",
    "    v_ij[:, 5] = H_stack[:, 2, i] * H_stack[:, 2, j]\n",
    "\n",
    "    return v_ij\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6095bdd-df3e-4c4d-9fc7-4224fd2c2d2a",
   "metadata": {},
   "source": [
    "### 4.2 Compute the intrisic Matrix.\n",
    "We need >= 3 homographies for a full 5-parameter intrinsic matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e3fe710-308e-4fae-b31a-d1b5a218ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrisicmatrix(H):\n",
    "    M = len(H)\n",
    "\n",
    "    V01 = generate_v(H,0,1)\n",
    "    V00 = generate_v(H,0,0)\n",
    "    V11 = generate_v(H,1,1)\n",
    "\n",
    "    # vq(H) is a 6 dimensional row vector obtained from the estimated Homography H\n",
    "    V = np.zeros((2*M,6))\n",
    "\n",
    "    V[:M] = V01\n",
    "    V[M:] = V00-V11\n",
    "\n",
    "    # Use SVD to solve the homogeneous system Vb = 0( In this case we have a overdetermined system)\n",
    "    U, S, Vh = np.linalg.svd(V)\n",
    "    idx = np.argmin(S)\n",
    "    b = Vh[idx]\n",
    "\n",
    "\n",
    "    # camera intrinsics( Zhang creates a new matrix which is symmetric and composed of only 6 distinct quantities)\n",
    "    B0, B1, B2, B3, B4, B5 = b\n",
    "\n",
    "    # B = A^(-T)*A^(-1)\n",
    "    B = np.array([[B0,B1,B3],\n",
    "                  [B1,B2,B4],\n",
    "                  [B3,B4,B5]]);\n",
    "    \n",
    "    print(B)\n",
    "\n",
    "    w = (B0*B2*B5) - ((B1**2)*(B5))-((B0)*(B4**2))+(2*B1*B3*B4)-((B2)*(B3**2))\n",
    "    d = B0*B2 - B1**2\n",
    "\n",
    "    v0 = (B[0,1] * B[0,2] - B[0,0] * B[1,2]) / (B[0,0] * B[1,1] - B[0,1]**2)\n",
    "    lambda_ = B[2,2] - (B[0,2]**2 + v0 * (B[0,1] * B[0,2] - B[0,0] * B[1,2])) / B[0,0]\n",
    "    print(lambda_)\n",
    "    alpha = np.sqrt(lambda_ / B[0,0])\n",
    "    beta = np.sqrt(lambda_ * B[0,0] / (B[0,0] * B[1,1] - B[0,1]**2))\n",
    "    gamma = -1 * (B[0,1] * alpha * alpha * beta )/ lambda_\n",
    "    u0 = (gamma * v0 / beta) - (B[0,2] * alpha * alpha / lambda_)\n",
    "\n",
    "\n",
    "    # Reconstitute intrinsic matrix\n",
    "    K = np.array([[alpha, gamma, u0],\n",
    "                  [   0,  beta, v0],\n",
    "                  [   0,    0, 1]])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88c4a60d-7c03-4759-b70c-759ea3ad8825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.23943447e-06 -2.77865433e-08 -5.25626059e-04]\n",
      " [-2.77865433e-08  2.16971798e-06 -5.62934738e-04]\n",
      " [-5.25626059e-04 -5.62934738e-04  9.99999703e-01]]\n",
      "0.7271466870400373\n",
      "[[ 2.06531760e-06  4.57399610e-10 -7.35511156e-04]\n",
      " [ 4.57399610e-10  2.04288266e-06 -4.85398023e-04]\n",
      " [-7.35511156e-04 -4.85398023e-04  9.99999612e-01]]\n",
      "0.6228103830208325\n"
     ]
    }
   ],
   "source": [
    "K_right = intrisicmatrix(H_refined_right)\n",
    "K_left = intrisicmatrix(H_refined_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6af5a926-ac75-40f6-a682-4761d2ae5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[569.82544189   7.18356529 237.97078499]\n",
      " [  0.         578.95375917 262.49822667]\n",
      " [  0.           0.           1.        ]]\n",
      "[[ 5.49141795e+02 -1.22282741e-01  3.56072360e+02]\n",
      " [ 0.00000000e+00  5.52148914e+02  2.37524732e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(K_right)\n",
    "print(K_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8722db2-a481-49a3-a1bc-7e12c21c6a09",
   "metadata": {},
   "source": [
    "## 5. Recover the extrinsic parameters\n",
    "After retrieving the intrinsic matrix, we use both the intrinsic and extrinsic matrix to calculate the corresponding extrinsic matrix( Burger, pag 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef9741-45d3-4b14-9bda-3b87fcd2e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_extrinsics(H,K):\n",
    "    # obtain the column vectores from the homography matrix\n",
    "    h0,h1,h2 = H[:,0],H[:,1],H[:,2]\n",
    "    \n",
    "    K_inv = np.linalg.inv(K)\n",
    "    \n",
    "    #norm the term\n",
    "    lambda_ = 1. / np.linalg.norm(np.dot(K_inv, h0))\n",
    "    \n",
    "    r0 = (lambda_ * np.dot(K_inv,h0))\n",
    "    r1 = (lambda_ * np.dot(K_inv,h1))\n",
    "    t =  (lambda_ * np.dot(K_inv,h2)).reshape(3,1)\n",
    "\n",
    "    # since R must be orthogonal\n",
    "    r2 = np.cross(r0,r1)\n",
    "    \n",
    "    # rotation matrix reconstrution\n",
    "    R = np.vstack((r0, r1, r2)).T\n",
    "    \n",
    "    # reorthogonalize the rotation matrix( Zhang, pag 18, Append C)\n",
    "    #(The sense of \"best\" rotation matrix R is in the sense of the smallest Frobenius norm of the difference R-Q)\n",
    "    U,S,Vt = np.linalg.svd(R)\n",
    "    R_ortho = np.dot(U,Vt)\n",
    "    extrinsics = np.hstack((R_ortho, t))\n",
    "    \n",
    "    return extrinsics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998c1ad-1569-414a-991a-4d73c8aa8c84",
   "metadata": {},
   "source": [
    "### 5.2 Compute the extrinsics based on the intrisincs already calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0467f-77c8-440e-9a17-7967bc476c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the extrinsic matrixes\n",
    "extrinsic_matrices = []\n",
    "\n",
    "\n",
    "# Homographies using the original intrinsic matrix\n",
    "for h, Homograph in enumerate(H):\n",
    "    E = extract_extrinsics(Homograph,K)\n",
    "    extrinsic_matrices.append(E)\n",
    "    \n",
    "    # Projection matrix\n",
    "    P = np.dot(K,E)\n",
    "    \n",
    "    # homogeneous world coordinates\n",
    "    homo_world = np.append(worldcoo[0][:].reshape(patternsize[0]*patternsize[1],3),np.ones((patternsize[0]*patternsize[1],1)),axis = 1) \n",
    "    temp_var = np.dot(homo_world,P.T)\n",
    "    # to euclidean coordinates\n",
    "    actual_coo = to_euclidean(temp_var)\n",
    "    predicted_coo = imagecoo[h]\n",
    "    \n",
    "    # least squares error\n",
    "    ls_error = np.sum(np.abs(actual_coo-predicted_coo)**2)\n",
    "    \n",
    "print(ls_error)    \n",
    "# Homographies using the refined intrinsic matrix    \n",
    "for h, Homograph in enumerate(H_refined):\n",
    "    E = extract_extrinsics(Homograph,K_refined)\n",
    "    extrinsic_matrices.append(E)\n",
    "    \n",
    "    # Projection matrix\n",
    "    P = np.dot(K_refined,E)\n",
    "    # homogeneous world coordinates\n",
    "    homo_world = np.append(worldcoo[0][:].reshape(patternsize[0]*patternsize[1],3),np.ones((patternsize[0]*patternsize[1],1)),axis = 1) \n",
    "    temp_var = np.dot(homo_world,P.T)\n",
    "    # to euclidean coordinates\n",
    "    actual_coo_LM = to_euclidean(temp_var)\n",
    "    predicted_coo = imagecoo[h]\n",
    "    \n",
    "    # least squares error\n",
    "    ls_error = np.sum(np.abs(actual_coo_LM-predicted_coo)**2)\n",
    "\n",
    "\n",
    "print\n",
    "print(ls_error)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-thesis]",
   "language": "python",
   "name": "conda-env-miniconda3-thesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
